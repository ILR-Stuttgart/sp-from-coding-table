---
---

```{r setup, include=FALSE}
library(ggplot2)
library(cowplot)
library(readxl)
library(tools)
knitr::opts_chunk$set(echo = TRUE)
# FILE PATHS

```

# How to use this script

This script applies the Sufficiency Principle to a table containing data
extracted from a corpus, where each row contains a relevant data-point.

The data frame, which is referenced in to in this script as `ct`, must contain
the following columns:

+ **type**: The name of the type represented in the row. For example, if you're
working with verbs, this would be the verb lemma. Treated as a factor.
+ **time**: Some kind of continuous variable which gives the time associated
with the data point, e.g. the year.
+ **group**: If the datapoints should be grouped (e.g. some are from the same
text) name of the group. Should be a factor.
+ **in_class**: Is the datapoint a token of the relevant class of items? **1**
  is true, **0** is false.
+ **follows_rule**: Does the datapoint follow the rule being evaluated? **1**
  is true, **0** is false.
  
If the dataset doesn't contain these columns, they can be generated using
the functions in the section [Global Functions](#global-functions).

# Setup

## Global variables

This block of R code is used to provide global variables to the script. 
**You should update these options.**

+ `CODING_TABLE`: Path to the file containing the data frame.
+ `WINDOW`: The maximum number of occurrences analysed by "synchronic" learners.
+ `NTRIM`: The number of verb lemmas known by "inexperienced" learners.

```{r globals}
# GLOBALS
CODING_TABLE <- path.expand("~/Google Drive Shared/digs2025-lability-project/combined-coding-table-v17-anim-spc-cos.xlsx")
WINDOW <- 100000 # number of clauses to allow in the "learning window" for the time limited learner
NTRIM <- 30 # how many verbs the inexperienced learner knows
```

## Global functions

Here are the function definitions which will be applied to generate the
**type**, **time** and **group** columns. They take the data frame as their
sole argument and return a data frame with these columns added. The default
functions do nothing. 

```{r global-functions}

########################
# DEFAULT FUNCTIONS    #
########################

make_type <- function(df) {
  df$type <- as.factor(as.character(df$type))
  return(df)
}

make_time <- function(df) {
  return(df)
}

make_group <- function(df) {
  df$group <- as.factor(as.character(df$group))
  return(df)
}

make_in_class <- function(df) {
  df$in_class <- as.factor(df$in_class)
}

make_follows_rule <- function(df) {
  df$follows_rule <- as.factor(df$follows_rule)
}

```

In the coding block below, you can **overwrite** the default functions 
with user-defined functions. This is necessary if the coding table doesn't
contain **type**, **time** or **group**.

```{r user-functions}
##########################
# FUNCTIONS FOR DIGS PAPER
##########################

usr_get_textid <- function(chars) {
  # If commas, split by comma
  if (grepl(",", chars)) {
    return(strsplit(
      chars, ",")[[1]][1]
    )
  } else {
    # else split by full stop
    return(strsplit(
      chars, ".")[[1]][1]
    )
  }
}

make_type <- function(df) {
  df$type <- as.factor(as.character(df$lemma))
  return(df)
}

make_time <- function(df) {
  df$time <- df$year
  return(df)
}

make_group <- function(df) {
  df$group <- as.factor(as.character(
      sapply(
        df$textid,
        usr_get_textid
      )
  ))
  return(df)
}

make_in_class <- function(df) {
  df$in_class <- as.factor(as.character(
    ifelse(as.character(df$cos) == "1", "1", "0")
  ))
  return(df)
}

make_follows_rule <- function(df) {
  df$follows_rule <- as.factor(as.character(
    ifelse(as.character(df$dobj) == "0", "0", "1")
  ))
  return(df)
}

```

## Preparing the dataset

The code block below:

1. loads the dataset.
2. adds the type, group and time columns
3. sorts the table by time and then by group

```{r prepare}

# Read df
if (file_ext(CODING_TABLE) %in% c("xls", "xlsx")) {
  ct <- read_excel(
    path=CODING_TABLE
  )
} else {
  ct <- read.delim(
    file=CODING_TABLE,
    quote="",
    stringsAsFactors=TRUE,
    header=TRUE,
    encoding="utf-8")
}

# Run functions

ct <- make_type(ct)
ct <- make_time(ct)
ct <- make_group(ct)
ct <- make_in_class(ct)
ct <- make_follows_rule(ct)

# Order df by time and group.
ct <- ct[
  order(ct$time, ct$group),
]

```

# Analysis

## Presentation

The data in the coding table is processed in chronological order 
using the **time** and **group** columns as the input to a learning algorithm.
The algorithm is trying to evaluate the evidence that a particular rule is
productive, considering the tokens of every **type** within a class (the
**in_class** column) to see whether or not they provide evidence for a
productive rule (the **follows_rule** column).

To model the experience of a language learner, let's imagine that the
learning algorithm "reads through" the texts in the corpus one by one.
For every occurrence of a item within the class in question,the learning
algorithm receives two pieces of data. First, it receives information
that the type exists. Second, it receives information as to whether it follows
a particular rule or not. As it "reads through" the corpus, it
acquires more and more data until it arrives at an analysis based on all
the corpus data.

However, there are a number of ways in which this *doesn't* model the
experience of a language learner. First, corpus data may contain a large 
number of different types, some of which are very rare and possibly
restricted to written registers. While a proficient speaker may
eventually acquire this many verbs, it's unclear whether many learners
will have been exposed to so many types during the
critical period. This is where "frequency trimming" comes in (Kodner
2019): we assume that a better approximation of the input to a real
language learner comes from ignore the rarer types, so we may want to
restrict our learning algorithm to focusing on only the most frequent
types. Second, historical data is spread out across a long time period, and it's
unlikely that any learner would have had access to such a wide range of
data chronologically. Consequently, as the learning algorithm
progresses through the corpus, data from further back in time should drop out of
the input.

The analysis thus models different types of learner based on two
independent parameters of variation:

* on the **diachronic** axis, we model a single **panchronic** learner, who has
access to data from every text, and a series of **synchronic** learners, who
only have access to data from a fixed number of clauses, given in the global
variable `WINDOW`.
* on the **knowledge** axis, we model an **experienced** learner, who bases their
analysis on every single type attested in the time period in question,
and an **inexperienced** learner, whose analysis is based only on the most
frequent types, given by the global variable `NTRIM`.

## Implementation

### Calculating cumulative frequency of data for each point in time

How much data is there cumulatively for each point in time?

```{r year cumul}
yt <- as.data.frame(table(ct$time)) # Create table of frequencies by year
colnames(yt) <- c("time", "clause.Freq")
yt$clause.cumFreq <- cumsum(yt$clause.Freq)
```

### Calculating class-size by year (N)

We next need to calculate how the value of *N* evolves over time, and we
do this by creating a table giving (i) the token frequencies of each
type by time, (ii) the cumulative token frequencies of each type by
time, and (iii) the cumulative token frequency of each type within the
window (for the time-limited learner).

We then rank the types for each time period by cumulative frequency and the
windowed cumulative frequency, so that we can allow frequency trimming.

Lastly, we generate binary variables to show whether this verb figures
in the vocabulary of our four learners in the year in question.

```{r cross-tabulation n}
nt <- as.data.frame(xtabs(
    formula = ~ time + type, # cross tabulate the factors
    data = ct, # from the ct data frame
    subset = as.character(ct$in_class) == "1", # subsetting for in_class types only
    drop.unused.levels = TRUE # and dropping levels
))
# From the year table, add the cumulative frequency of clauses up to this year
nt <- merge(nt, yt[,c("time", "clause.cumFreq")], by="time", sort=FALSE)
# Resort the df, making it readable.
nt <- nt[order(nt$type, nt$time),]
row.names(nt) <- NULL # reset the row indices so it stays in the right order
# The cross-tabulation sets the year as a factor, then says it's an integer.
# Let's eliminate this problematic behaviour
nt$time <- as.numeric(as.character(nt$time))
#############################################
# Calculate cumulative frequencies and cumsum.window 
# Yes it is far too complicated to use an apply function here.
x <- c() # set two empty return vectors
y <- c() 
for (i in 1:nrow(nt)) { # loop over row indices
  # first, sum the rows from mt with the same lemma and the same transitivity
  # dating from year or earlier
  qwe <- nt[nt$type == nt[i,]$type & nt$time <= nt[i,]$time,]
  # Calculate cumfreq, add it to x
  x <- c(x, sum(qwe$Freq))
  # Further subset qwe to exclude clause.cumFreqs that are WINDOW less than the current one.
  y <- c(y, sum(qwe[qwe$clause.cumFreq > nt[i,]$clause.cumFreq - WINDOW,]$Freq))
 }
nt$cumFreq <- x # Set cumFreq from x
nt$cumFreq.window <- y # Set cumFreq.window from y
############################################
# Ranking the cumfreq values
nt$cumFreq.rank <- ave(nt$cumFreq, nt$time, FUN = \(x) rank(x * -1, ties.method = "random"))
nt$cumFreq.window.rank <- ave(nt$cumFreq.window, nt$time, FUN = \(x) rank(x * -1, ties.method = "random"))
############################################
summary(nt)
```

### Calculating attested structures for each verb by year (*M*)

Next, we need to apply the same procedure but including the **follows_rule**
information too. This table forms the basis of the analysis. Each row
codes a type-structure pair.

The columns in the data frame give the following information about
**token** frequency: 

* `Freq`: frequency of the type-structure pair *in
this year only* 
* `cumFreq`: cumulative frequency of the type-structure
pair *up to and including this year* 
* `cumFreq.rank`: ranking of the
relative frequency of the type-structure pair up to and including this
year, where 1 is the most frequent. 
* `clause.cumFreq`: how many clauses
occur in the corpus up to and including this year 
* `cumFreq.window`:
cumulative frequency of the type-structure pair up to and including
this time point, but only considering the previous `WINDOW` clauses in the
corpus. 
* `cumFreq.window.rank`: ranking of the relative frequency of
type-structure pairs up to and including this point in time, but only
considering the previous `WINDOW` clauses in the corpus where 1 is the
most frequent. 
* `window.start.year`: since the window is based on the
previous `WINDOW` clauses, this records the earliest time point of the in
the window.

A non-trivial problem is that for trimming the lexicon, we need to know
how many *types* the learner knows rather than how many type-structure
pairs. To illustrate this problem and how we solve it, let's imagine
that a very inexperienced learner has only learned the following five
most frequent "types":

1. *loven* + not rule following;
2. *dreden* + not rule following;
3. *admire* + not rule following;
4. *plesen* + not rule following;
5. *haten* + not rule following;
6. *dreden* + rule following.

Although the table rankings show that this
learner has learned six structures, all of which are (potentially)
relevant when trying to calculate the value of *M* for the SP, the
learner in fact only knows five types. So the table needs to indicate,
for each row, how many type. the learner has acquired by the time they
learn the type-structure pair coded in the row. This is done by the following
columns: 

* `cumFreq.rank.n`: number of types that the learner has
already encountered by the time they learn this particular
type-structure pair. In the example above, the value for both *haten* +
not rule following and *dreden* + rule following would be 4, since learning the
*dreden* rule-following type-structure pair doesn't involve learning a
new type. 
* `cumFreq.window.rank.n`: as above, but only considering
data from the last `WINDOW` clauses in the corpus.

Using these values, we can now create four boolean variables to indicate
which of our four learners would have been exposed to the
lemma-structure pair in the table at that particular date: 

* `is.learner.ep`: the experienced, panchronic learner, i.e. the learner
whose input is everything attested in the corpus 
* `is.learner.es`: the
experienced, synchronic learner, i.e. the learner whose input is
everything attested in the previous `WINDOW` clauses 
* `is.learner.ip`:
the inexperienced, panchronic leaner, i.e. the learner whose input is
the most frequent type-structure pairs in the corpus up to this point
up until a maxmimum of `NTRIM` verbs have been learned. 
* `is.learner.is`:
the inexperienced, synchronic learner, as above, but calculated from the
last `WINDOW` clauses rather than the full corpus.

**Important technical note: this code block can be very slow if there is
a lot of data. We're talking hours.**

```{r cross-tabulation m}
mt <- as.data.frame(xtabs(
    formula = ~ time + type + follows_rule, # cross tabulate the factors
    data = ct, # from the ct data frame
    subset = ct$in_class == "1", # subsetting for in_class types only
    drop.unused.levels = TRUE # and dropping levels
))
# From the year table, add the cumulative frequency of clauses up to this year
mt <- merge(mt, yt[,c("time", "clause.cumFreq")], by="time", sort=FALSE)
# Resort the df, making it readable.
mt <- mt[order(mt$follows_rule, mt$type, mt$time),]
row.names(mt) <- NULL # reset the row indices so it stays in the right order
# The cross-tabulation sets the year as a factor, then says it's an integer.
# Let's eliminate this problematic behaviour
mt$time <- as.numeric(as.character(mt$time))
# Calculate cumulative frequencies and cumsum.window 
# Yes it is far too complicated to use an apply function here.
x <- c() # set three empty return vectors
y <- c() 
z <- c()
for (i in 1:nrow(mt)) { # loop over row indices
  # first, sum the rows from mt with the same type and the same follows_rule
  # dating from time or earlier
  qwe <- mt[mt$type == mt[i,]$type & mt$follows_rule == mt[i,]$follows_rule & mt$time <= mt[i,]$time,]
  # Calculate cumfreq, add it to x
  x <- c(x, sum(qwe$Freq))
  # Further subset qwe to exclude clause.cumFreqs that are WINDOW less than the current one.
  asd <- qwe[qwe$clause.cumFreq > mt[i,]$clause.cumFreq - WINDOW,]
  y <- c(y, sum(asd$Freq))
  # Let's also record the year the window starts (for future reference)
  z <- c(z, min(asd$time))
 }
mt$cumFreq <- x # Set cumFreq from x
mt$cumFreq.window <- y # Set cumFreq.window from y
mt$window.start.year <- z # Set startyear from z
############################################
# Ranking the cumfreq values
mt$cumFreq.rank <- ave(mt$cumFreq, mt$time, FUN = \(x) rank(x * -1, ties.method = "random"))
mt$cumFreq.window.rank <- ave(mt$cumFreq.window, mt$time, FUN = \(x) rank(x * -1, ties.method = "random"))
########################################################
# Calculating N for the window.rank scores for each year
# For each row, we subset the lemma vector by year and cumFreq.window.rank.
# We then calculate how many levels the new factor has.
mt$cumFreq.window.rank.n <- 0
mt$cumFreq.rank.n <- 0
for (i in 1:nrow(mt)) { # iterate over the data frame
  # subset lemmas by year and having a lower or equal cumFreq.(window).rank
  x <- mt[mt$time == mt[i,]$time & mt$cumFreq.window.rank <= mt[i,]$cumFreq.window.rank,]$type
  y <- mt[mt$time == mt[i,]$time & mt$cumFreq.rank <= mt[i,]$cumFreq.rank,]$type
  # calculate number of lemmas known (levels of the new vector)
  mt[i,]$cumFreq.window.rank.n <- length(levels(droplevels(x)))
  mt[i,]$cumFreq.rank.n <- length(levels(droplevels(y)))
}

# Resort the df (again).
mt <- mt[order(mt$follows_rule, mt$type, mt$time),]
row.names(mt) <- NULL # reset the row indices so it stays in the right order
############################################################
# Calculate whether the type is in the vocab of each learner
###########################################################
mt$is.learner.ep <- ifelse(mt$cumFreq > 0, TRUE, FALSE) # experienced, panchronic
mt$is.learner.es <- ifelse(mt$cumFreq.window > 0, TRUE, FALSE) # experienced, time-limited
mt$is.learner.ip <- ifelse(mt$cumFreq.rank.n <= NTRIM & mt$cumFreq > 0, TRUE, FALSE) # inexperienced, panchronic
mt$is.learner.is <- ifelse(mt$cumFreq.window.rank.n <= NTRIM & mt$cumFreq.window > 0, TRUE, FALSE) # inexperienced, time-limited
#########################################################
summary(mt)

```

### Calculating values for N, M and the SP for each year.

We now build a data table showing the results of a SP analysis for each learner
and for each point in time.

```{r tp table}
# values for n
#mt$tp.learner.ep.n <- ave(mt$cumFreq.rank.n, as.factor(mt$year), as.factor(mt$is.learner.ep), FUN = max)
#mt$tp.learner.es.n <- ave(mt$cumFreq.window.rank.n, as.factor(mt$year), as.factor(mt$is.learner.es), FUN = max)
#mt$tp.learner.ip.n <- ave(mt$cumFreq.rank.n, as.factor(mt$year), as.factor(mt$is.learner.ip), FUN = max) # problem is rank 30 is joint
#mt$tp.learner.is.n <- ave(mt$cumFreq.window.rank.n, as.factor(mt$year), as.factor(mt$is.learner.is), FUN = max)
# values for m
#mt$tp.learner.ep.m.amuse <- ave(mt$lemma, as.factor(mt$year), as.factor(mt$is.learner.ep), as.factor(mt$is.m.amuse), FUN = length)
times <-levels(as.factor(ct$time))
tpt <- data.frame(matrix(ncol=4, nrow=0))
# Generate SP stats. This is very slow and inefficient code, but it's not a simple
# filtering operation.
############
# EP learner
qwe <- subset(mt, is.learner.ep)
n <- tapply(qwe$cumFreq.rank.n, as.factor(qwe$time), FUN = max)
m <- tapply(qwe[qwe$follows_rule == "1",]$type, as.factor(qwe[qwe$follows_rule == "1",]$time), FUN = \(x) length(levels(droplevels(x))))
tpt <- rbind(tpt, cbind(times, "ep", n, m))
##############
# ES learner
qwe <- subset(mt, is.learner.es)
n <- tapply(qwe$cumFreq.window.rank.n, as.factor(qwe$time), FUN = max)
m <- tapply(qwe[qwe$follows_rule == "1",]$type, as.factor(qwe[qwe$follows_rule == "1",]$time), FUN = \(x) length(levels(droplevels(x))))
tpt <- rbind(tpt, cbind(times, "es", n, m))
###############
# IP learner
qwe <- subset(mt, is.learner.ip)
n <- tapply(qwe$cumFreq.rank.n, as.factor(qwe$time), FUN = max)
m <- tapply(qwe[qwe$follows_rule == "1",]$type, as.factor(qwe[qwe$follows_rule == "1",]$time), FUN = \(x) length(levels(droplevels(x))))
tpt <- rbind(tpt, cbind(times, "ip", n, m))
###############
# IS learner
qwe <- subset(mt, is.learner.is)
n <- tapply(qwe$cumFreq.window.rank.n, as.factor(qwe$time), FUN = max)
m <- tapply(qwe[qwe$follows_rule == "1",]$type, as.factor(qwe[qwe$follows_rule == "1",]$time), FUN = \(x) length(levels(droplevels(x))))
tpt <- rbind(tpt, cbind(times, "is", n, m))
###############
# SP calcs
colnames(tpt) <- c("time", "learner", "n", "m")
tpt$n <- as.numeric(tpt$n)
tpt$m <- as.numeric(tpt$m)
tpt$time <- as.numeric(tpt$time)
tpt$theta.n <- tpt$n / log(tpt$n)
tpt$n.minus.m <- tpt$n - tpt$m
tpt$sufficient <- ifelse(tpt$n.minus.m < tpt$theta.n, TRUE, FALSE)
tpt$sufficient.0 <- tpt$n.minus.m - tpt$theta.n
summary(tpt)

# qwe <- subset(tpt, learner == "is" & rule == "is.sbjexp")
# plot(qwe[,c("year","n","m", "sufficient.0")])

```

**UPDATE** Since the initial table is OK to read but lousy to plot, we
turn it into a plottable table.

```{r tpt.plot}
library(data.table)
# Panchronic learners
# Bash the data into the right format to graph it (tip: all type values need
# to be in a single column.)
# Theta N needs to be both in the Stat column and a separate value
n <- tpt[,c("time", "learner", "theta.n", "n")]
n$stat <-"N"
m <- tpt[,c("time", "learner", "theta.n", "m")]
m$stat <- "M"
n.minus.m <- tpt[,c("time", "learner", "theta.n", "n.minus.m")]
n.minus.m$stat <- "N minus M"
theta.n <- tpt[,c("time", "learner", "theta.n", "theta.n")]
theta.n$stat <- "Theta N"
# Now we need to RENAME the column because rbind requires them all to be
# named
l <- lapply(
  list(n, m, n.minus.m, theta.n), # for each item in the list
  \(x) setnames(x, c("Time", "Learner", "theta.n", "Types", "Stat")) #use setnames
)
tpt.plot <- as.data.frame(unclass(do.call(rbind, l)), stringsAsFactors=TRUE) # rbind the dfs together
summary(tpt.plot)
```

# Results

## Subject experiencers

### Panchronic learners

First, we'll consider how the two panchronic learners got on learning
subject experiencers, comparing the results with those of
Trips/Rainsford (2022).

Here's the R code to generate the plots we're going to need (no output).

```{r panchronic}
##############################################################################
# Define some functions to draw the plots.
# N and M plot
nmplot <- function(learner, rule, df) {
  # Turn year to a factor
  df$Year <- as.factor(df$Year)
  # Subset the data
  df <- droplevels(subset(df, Learner == learner & Rule == rule & Year %in% c("1400", "1500") & Stat %in% c("M", "N minus M")))
  qwe <- ggplot(
    data = df, # set the dataset
    mapping = aes(Year, Types, fill = Stat) # Select x and y coordinates; tell it use the fill value to set colour
  ) + 
  geom_bar( # tell it to make a bar plot
    position="stack", # tell it to stack the columns
    stat="identity" # tell it to use the raw value
  ) +
  geom_point( # Add a point for the threshold value
    data = df, # using theta_N data
    mapping = aes(Year, theta.n), # to add a point for the threshold
    size = 2, # change its size
    show.legend = FALSE # no legend required
  ) +
  geom_text( # Add label
    data = df, # using theta_N data
    mapping = aes(Year, theta.n), # to add a label for the threshold
    label = "Theta N", # Label
    nudge_y = 7 # move it up a bit
  ) +
  theme_minimal() + # Use the minimal theme
  labs(x="Date of latest text", y="Number of types") 
  return(qwe)
}

ep.sbjexp <- nmplot(learner="ep", rule="is.sbjexp", df=tpt.plot) + ylim(NA, 130)
ip.sbjexp <- nmplot(learner="ip", rule="is.sbjexp", df=tpt.plot) + ylim(NA, 130)
ep.sbjexp.tr22 <- nmplot(learner="ep", rule="is.sbjexp.tr22", df=tpt.plot) + ylim(NA, 130)
ip.sbjexp.tr22 <- nmplot(learner="ip", rule="is.sbjexp.tr22", df=tpt.plot) + ylim(NA, 130)
```

Below we see how our panchronic learners perform if we "stop the clock"
in 1400 (as Trips/Rainsford 2022 did) and if we use all the data in the
corpus, which takes us up to 1500.

```{r panchronic plots corpus m}
plot_grid(
  ep.sbjexp, ip.sbjexp,
  ncol = 2, # two columns
  nrow = 1,
  labels = c("Full lexicon", paste("Top ", as.character(NTRIM), " psych verbs")) # Plot titles
)
```

Neither of the panchronic learners acquires a productive subject
experiencer rule. However, the learner who uses the full corpus is
closer to doing so in 1500 than in 1400. In the 15th century, she learns
nine more verbs, but comes across fourteen more cases of verbs with a
subject experiencer. This hints that subject experiencer uses of
pre-existing verbs are first found in the 15th century.

The previous graph followed our new methodology and calculated *M* from
the corpus. But what happens if we used the MED to estimate the value of
*M*, as Trips and Rainsford (2022) did; i.e., what happens if we simply
use the corpus data to estimate the value of *N*?

```{r panchronic plots med m}
plot_grid(
  ep.sbjexp.tr22, ip.sbjexp.tr22,
  ncol = 2, # two columns
  nrow = 1,
  labels = c("Full lexicon", paste("Top ", as.character(NTRIM), " psych verbs")) # Plot titles
)
```

As we can see, this time we're able to replicate the result from
Trips/Rainsford (2022) with the new corpus. We need to look further into
this, but it seems like some of the subject experiencers uses of psych
verbs in the MED aren't being found in the corpus.

## Synchronic learners

Next, we're going to take a look at the two "synchronic" (ish) learners.
Here's the R code for generating the plots.

```{r diachronic learners}

# First, calculate the point from which it's sensible to start plotting, i.e.
# the year from which we have more than WINDOW clauses
start.year <- min(mt[mt$clause.cumFreq > WINDOW,]$year)
# Trim the data table
tpt.trimmed <- subset(tpt.plot, Year >= start.year)

nmplot.time <- function(learner, rule, df) {
  # Subset the data
  df <- droplevels(subset(df, Learner == learner & Rule == rule))
  qwe <- ggplot(
    data = df, # set the dataset
    mapping = aes(Year, Types)
  ) +
  geom_line( # tell it to make a line graph
    mapping = aes(color = Stat), # grouping and colouring by Stat
    #show.legend = TRUE
  ) + 
  ylim(0, NA) + # Making sure the scale start from zero
  theme_minimal() + # Use the minimal theme
  labs(x="Date of latest text", y="Number of types")
  return(qwe)
}

es.sbjexp <- nmplot.time(learner="es", rule="is.sbjexp", tpt.trimmed)
is.sbjexp <- nmplot.time(learner="is", rule="is.sbjexp", tpt.trimmed)
es.sbjexp.tr22 <- nmplot.time(learner="es", rule="is.sbjexp.tr22", tpt.trimmed)
is.sbjexp.tr22 <- nmplot.time(learner="is", rule="is.sbjexp.tr22", tpt.trimmed)

```

In each year, the diachronic learner is only drawing data from the last
100,000 clauses. (The plot starts in 1315, which is the earliest data
before which there are 100,000 clauses in the corpus). As time goes by,
the learner add new data and forgets old data. So we can see how
learners of different generations may have come to different conclusions
based on the input.

```{r synchronic corpus m}
plot_grid(
  es.sbjexp, is.sbjexp,
  ncol = 2, # two columns
  nrow = 1,
  labels = c("Full lexicon", paste("Top ", as.character(NTRIM), " psych verbs")) # Plot titles
)
```

The headline result is that the synchronic learners come to the same
conclusion as the panchronic learners: there's no productive subject
experiencer rule, since the line showing *N minus M* never drops below
the line for *Theta N*. However, there's something going on towards the
end of the period. The learner using the full lexicon starts to know
fewer psych verbs (*N* reduces), suggesting that some of the Old English
vocabulary is now being lost. At the same time, she knows *more* psych
verbs with subject experiencers (*M* increases), suggesting that subject
experiencers are become more frequent in the input. As a result, she
suddenly comes back really close to the threshold for acquiring a
productive rule. The learner using a frequency trimmed corpus obviously
knows only 30 verbs throughout the time period. However, the value of
*M* jumps up at the end of the period, suggesting either that she is
learning more subject-experiencer verbs, or that the verbs she already
know are being used more frequently with subject experiencers. Once
again, she never quite crosses the TP threshold.

If we apply Trips/Rainsford's (2022) methodology for calculating *M*,
how does the picture change?

```{r synchronic med m}
plot_grid(
  es.sbjexp.tr22, is.sbjexp.tr22,
  ncol = 2, # two columns
  nrow = 1,
  labels = c("Full lexicon", paste("Top ", as.character(NTRIM), " psych verbs")) # Plot titles
)
```

A very different picture emerges when the MED data is used to estimate
M. For the full lexicon, the difference between *N* and *M* stays pretty
constantly just below the threshold. For the frequency-trimmed lexicon,
the number of exceptions is definitely below the threshold. This
replicates Trips and Rainsford's (2022) result.

What does this mean? In short, it means that a lot of verbs that the MED
records as taking subject-experiencers are not actually attested with
this argument structure in the corpus. Moreover, the discrepancy between
the two values of *M* is much bigger in the earliest years of the corpus
data, which suggests (i) that new verbs are initially attested only with
a transitive, *amuse*-type argument structure and/or (ii) older verbs
which are reported as taking subject-experiencers are never attested as
doing so.

# Attested argument structure of psych verbs

(This section old, no longer operational.)

Using the coding table, calculate frequency of intransitive and
transitive use for each verb, adding this to the table of psych verbs
created for the paper. Distinguish different types of objects for each
verb.

```{r arg structure}
# cross tabulate lemmas and dobj values. Use as.data.frame.matrix
#qwe <- as.data.frame.matrix(table(df$lemma, df$dobj))
# prefix for colnames
#colnames(qwe) <- paste("dobj.", colnames(qwe), sep="")
# add a sum of non-transitive columns (2 to 5)
#qwe$dobj.trans <- apply(qwe[,2:5], 1, sum)
# calculate percent transitive
#qwe$dobj.trans.ratio <- qwe$dobj.trans / (qwe$dobj.trans + qwe$dobj.0)
# merge datasets
#psych_verbs_as <- merge(df.pv, qwe, by.x = "lemma", by.y = "row.names")
# save
#write.csv(psych_verbs_as, OUT_AS)
```
